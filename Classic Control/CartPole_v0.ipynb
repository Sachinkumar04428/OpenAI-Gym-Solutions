{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImJOfeUMr3g1",
        "colab_type": "text"
      },
      "source": [
        "## CartPole-v0\n",
        "We will solve CartPole-v0 environment using Reinforce Algorithm, which is a Poliicy Gradient method.\n",
        "\n",
        "### Reinforce Algorithm\n",
        "\n",
        "pseudocode:\n",
        "\n",
        "1. Use the policy $\\pi_{\\theta}$ to collect $m$ trajectories $\\tau^{1}$... $\\tau^{n}$.\n",
        "2. Use the trajectories to estimate the gradient $\\nabla_{\\theta}U(\\theta)$.\n",
        "\n",
        "     $\\nabla_\\theta U(\\theta): \\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})$\n",
        "3. Updte the weights of the policy $\\theta = \\theta + \\alpha g$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0S71kR_k7Rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) \n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6LlbH8tpYVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "323bcd49-e58e-40be-88fd-8c7e8b837115"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.seed(0)\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(4,)\n",
            "action space: Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq03Cj1blO0I",
        "colab_type": "text"
      },
      "source": [
        "### Loss\n",
        "\n",
        "Now, we are evaluating the action chosen by the policy for a given state by multiplying the log probablity of the action for that state with the cumulative discounted reward for that trajectory $\\tau^{(i)}$. \n",
        "\n",
        "$\\nabla_\\theta U(\\theta): \\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})$\n",
        "\n",
        "Hence $\\hat{g}$ becomes our loss function and we must backprop from here. \n",
        "The ***policy_loss.backward()*** line denotes this fact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb91w6ZXJ-GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
        "    solved = False\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        # Calculate the discounts\n",
        "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
        "        # Find the cumulative reward for trajectory\n",
        "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
        "        \n",
        "        policy_loss = []\n",
        "        # Calculat g<hat> \n",
        "        for log_prob in saved_log_probs:\n",
        "            # Note that we have to perform Gradient Ascent, to get the optimal solution\n",
        "            # Hence we will do gradient descent on -g<hat> , giving us the same solution\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()   # back prop\n",
        "        optimizer.step()         # gradient descent   \n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        if np.mean(scores_deque)>=195.0:\n",
        "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
        "            solved = True\n",
        "            break\n",
        "        \n",
        "    return scores, solved, i_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRSNwM52Jd9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7f318a0-9f13-49be-ee14-46505db03234"
      },
      "source": [
        "episodes = []\n",
        "for i in range(20):\n",
        "    torch.manual_seed(i)\n",
        "    policy = Policy().to(device)   # Initialising policy\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)  # Initialising optimizer\n",
        "    scores, solved, i_episode = reinforce(policy, optimizer)\n",
        "    if solved:\n",
        "        episodes.append(i_episode)\n",
        "    else:\n",
        "        episodes.append(0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 35.04\n",
            "Episode 200\tAverage Score: 55.56\n",
            "Episode 300\tAverage Score: 67.36\n",
            "Episode 400\tAverage Score: 93.81\n",
            "Episode 500\tAverage Score: 159.75\n",
            "Episode 600\tAverage Score: 139.53\n",
            "Episode 700\tAverage Score: 71.76\n",
            "Episode 800\tAverage Score: 135.34\n",
            "Episode 900\tAverage Score: 139.74\n",
            "Episode 1000\tAverage Score: 131.99\n",
            "Episode 100\tAverage Score: 32.28\n",
            "Episode 200\tAverage Score: 49.51\n",
            "Episode 300\tAverage Score: 62.64\n",
            "Episode 400\tAverage Score: 49.66\n",
            "Episode 500\tAverage Score: 58.09\n",
            "Episode 600\tAverage Score: 139.67\n",
            "Episode 700\tAverage Score: 145.74\n",
            "Episode 800\tAverage Score: 170.68\n",
            "Environment solved in 753 episodes!\tAverage Score: 195.25\n",
            "Episode 100\tAverage Score: 25.28\n",
            "Episode 200\tAverage Score: 57.27\n",
            "Episode 300\tAverage Score: 93.00\n",
            "Episode 400\tAverage Score: 59.86\n",
            "Episode 500\tAverage Score: 85.10\n",
            "Episode 600\tAverage Score: 77.03\n",
            "Episode 700\tAverage Score: 97.94\n",
            "Episode 800\tAverage Score: 98.75\n",
            "Episode 900\tAverage Score: 128.68\n",
            "Episode 1000\tAverage Score: 144.56\n",
            "Episode 100\tAverage Score: 30.11\n",
            "Episode 200\tAverage Score: 28.74\n",
            "Episode 300\tAverage Score: 55.02\n",
            "Episode 400\tAverage Score: 41.32\n",
            "Episode 500\tAverage Score: 47.79\n",
            "Episode 600\tAverage Score: 85.88\n",
            "Episode 700\tAverage Score: 86.59\n",
            "Episode 800\tAverage Score: 59.62\n",
            "Episode 900\tAverage Score: 93.78\n",
            "Episode 1000\tAverage Score: 109.10\n",
            "Episode 100\tAverage Score: 28.59\n",
            "Episode 200\tAverage Score: 50.22\n",
            "Episode 300\tAverage Score: 48.96\n",
            "Episode 400\tAverage Score: 89.26\n",
            "Episode 500\tAverage Score: 90.99\n",
            "Episode 600\tAverage Score: 68.43\n",
            "Episode 700\tAverage Score: 104.41\n",
            "Episode 800\tAverage Score: 80.54\n",
            "Episode 900\tAverage Score: 82.35\n",
            "Episode 1000\tAverage Score: 104.69\n",
            "Episode 100\tAverage Score: 22.45\n",
            "Episode 200\tAverage Score: 25.85\n",
            "Episode 300\tAverage Score: 32.78\n",
            "Episode 400\tAverage Score: 47.67\n",
            "Episode 500\tAverage Score: 41.23\n",
            "Episode 600\tAverage Score: 45.70\n",
            "Episode 700\tAverage Score: 53.40\n",
            "Episode 800\tAverage Score: 60.62\n",
            "Episode 900\tAverage Score: 46.75\n",
            "Episode 1000\tAverage Score: 36.47\n",
            "Episode 100\tAverage Score: 20.71\n",
            "Episode 200\tAverage Score: 42.12\n",
            "Episode 300\tAverage Score: 75.95\n",
            "Episode 400\tAverage Score: 53.66\n",
            "Episode 500\tAverage Score: 40.45\n",
            "Episode 600\tAverage Score: 91.25\n",
            "Episode 700\tAverage Score: 105.22\n",
            "Episode 800\tAverage Score: 100.15\n",
            "Episode 900\tAverage Score: 91.49\n",
            "Episode 1000\tAverage Score: 120.68\n",
            "Episode 100\tAverage Score: 27.44\n",
            "Episode 200\tAverage Score: 44.64\n",
            "Episode 300\tAverage Score: 168.51\n",
            "Episode 400\tAverage Score: 85.94\n",
            "Episode 500\tAverage Score: 179.49\n",
            "Environment solved in 492 episodes!\tAverage Score: 195.30\n",
            "Episode 100\tAverage Score: 43.17\n",
            "Episode 200\tAverage Score: 65.70\n",
            "Episode 300\tAverage Score: 64.11\n",
            "Episode 400\tAverage Score: 82.65\n",
            "Episode 500\tAverage Score: 75.80\n",
            "Episode 600\tAverage Score: 61.16\n",
            "Episode 700\tAverage Score: 64.06\n",
            "Episode 800\tAverage Score: 68.95\n",
            "Episode 900\tAverage Score: 106.15\n",
            "Episode 1000\tAverage Score: 48.26\n",
            "Episode 100\tAverage Score: 28.16\n",
            "Episode 200\tAverage Score: 37.32\n",
            "Episode 300\tAverage Score: 39.74\n",
            "Episode 400\tAverage Score: 54.53\n",
            "Episode 500\tAverage Score: 50.11\n",
            "Episode 600\tAverage Score: 55.18\n",
            "Episode 700\tAverage Score: 88.96\n",
            "Episode 800\tAverage Score: 76.46\n",
            "Episode 900\tAverage Score: 78.72\n",
            "Episode 1000\tAverage Score: 52.53\n",
            "Episode 100\tAverage Score: 22.53\n",
            "Episode 200\tAverage Score: 70.56\n",
            "Episode 300\tAverage Score: 123.15\n",
            "Episode 400\tAverage Score: 135.40\n",
            "Episode 500\tAverage Score: 165.28\n",
            "Episode 600\tAverage Score: 137.48\n",
            "Episode 700\tAverage Score: 91.27\n",
            "Episode 800\tAverage Score: 169.39\n",
            "Episode 900\tAverage Score: 176.54\n",
            "Episode 1000\tAverage Score: 101.65\n",
            "Episode 100\tAverage Score: 18.17\n",
            "Episode 200\tAverage Score: 29.86\n",
            "Episode 300\tAverage Score: 27.78\n",
            "Episode 400\tAverage Score: 40.29\n",
            "Episode 500\tAverage Score: 38.33\n",
            "Episode 600\tAverage Score: 70.55\n",
            "Episode 700\tAverage Score: 55.75\n",
            "Episode 800\tAverage Score: 43.70\n",
            "Episode 900\tAverage Score: 29.77\n",
            "Episode 1000\tAverage Score: 33.06\n",
            "Episode 100\tAverage Score: 25.80\n",
            "Episode 200\tAverage Score: 41.61\n",
            "Episode 300\tAverage Score: 57.06\n",
            "Episode 400\tAverage Score: 61.33\n",
            "Episode 500\tAverage Score: 58.52\n",
            "Episode 600\tAverage Score: 145.00\n",
            "Episode 700\tAverage Score: 118.61\n",
            "Episode 800\tAverage Score: 139.03\n",
            "Episode 900\tAverage Score: 63.63\n",
            "Episode 1000\tAverage Score: 77.66\n",
            "Episode 100\tAverage Score: 20.39\n",
            "Episode 200\tAverage Score: 31.58\n",
            "Episode 300\tAverage Score: 51.27\n",
            "Episode 400\tAverage Score: 62.03\n",
            "Episode 500\tAverage Score: 99.43\n",
            "Episode 600\tAverage Score: 100.58\n",
            "Episode 700\tAverage Score: 127.14\n",
            "Episode 800\tAverage Score: 106.02\n",
            "Episode 900\tAverage Score: 102.98\n",
            "Episode 1000\tAverage Score: 98.93\n",
            "Episode 100\tAverage Score: 29.12\n",
            "Episode 200\tAverage Score: 40.99\n",
            "Episode 300\tAverage Score: 38.33\n",
            "Episode 400\tAverage Score: 58.68\n",
            "Episode 500\tAverage Score: 79.27\n",
            "Episode 600\tAverage Score: 67.27\n",
            "Episode 700\tAverage Score: 111.89\n",
            "Episode 800\tAverage Score: 106.78\n",
            "Episode 900\tAverage Score: 62.56\n",
            "Episode 1000\tAverage Score: 69.02\n",
            "Episode 100\tAverage Score: 28.41\n",
            "Episode 200\tAverage Score: 38.79\n",
            "Episode 300\tAverage Score: 75.06\n",
            "Episode 400\tAverage Score: 42.92\n",
            "Episode 500\tAverage Score: 42.18\n",
            "Episode 600\tAverage Score: 46.20\n",
            "Episode 700\tAverage Score: 29.54\n",
            "Episode 800\tAverage Score: 55.36\n",
            "Episode 900\tAverage Score: 50.03\n",
            "Episode 1000\tAverage Score: 42.07\n",
            "Episode 100\tAverage Score: 28.02\n",
            "Episode 200\tAverage Score: 56.79\n",
            "Episode 300\tAverage Score: 62.81\n",
            "Episode 400\tAverage Score: 73.98\n",
            "Episode 500\tAverage Score: 85.18\n",
            "Episode 600\tAverage Score: 58.32\n",
            "Episode 700\tAverage Score: 81.25\n",
            "Episode 800\tAverage Score: 63.74\n",
            "Episode 900\tAverage Score: 63.49\n",
            "Episode 1000\tAverage Score: 71.34\n",
            "Episode 100\tAverage Score: 19.61\n",
            "Episode 200\tAverage Score: 21.55\n",
            "Episode 300\tAverage Score: 26.45\n",
            "Episode 400\tAverage Score: 42.93\n",
            "Episode 500\tAverage Score: 73.33\n",
            "Episode 600\tAverage Score: 90.57\n",
            "Episode 700\tAverage Score: 178.42\n",
            "Environment solved in 640 episodes!\tAverage Score: 195.19\n",
            "Episode 100\tAverage Score: 25.46\n",
            "Episode 200\tAverage Score: 45.18\n",
            "Episode 300\tAverage Score: 48.48\n",
            "Episode 400\tAverage Score: 53.69\n",
            "Episode 500\tAverage Score: 92.24\n",
            "Episode 600\tAverage Score: 107.17\n",
            "Episode 700\tAverage Score: 110.22\n",
            "Episode 800\tAverage Score: 190.27\n",
            "Environment solved in 706 episodes!\tAverage Score: 195.45\n",
            "Episode 100\tAverage Score: 35.21\n",
            "Episode 200\tAverage Score: 51.38\n",
            "Episode 300\tAverage Score: 55.16\n",
            "Episode 400\tAverage Score: 36.67\n",
            "Episode 500\tAverage Score: 34.89\n",
            "Episode 600\tAverage Score: 62.92\n",
            "Episode 700\tAverage Score: 67.08\n",
            "Episode 800\tAverage Score: 70.01\n",
            "Episode 900\tAverage Score: 62.06\n",
            "Episode 1000\tAverage Score: 56.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGOCpffBqTy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e89470a0-cc4e-4c98-b73c-1b7d456a90df"
      },
      "source": [
        "# Plotting the intial seed vs num episodes to solve the env\n",
        "plt.bar(np.arange(len(episodes)), episodes)\n",
        "plt.xlabel('seed')\n",
        "plt.ylabel('Num episodes to solve')\n",
        "plt.xticks(np.arange(20))\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHTJJREFUeJzt3XuYHVWZ7/Hvj1y4S7i0GUyCAYl4\nPYmxxTAiMxJRQCUMRxAHNWLOyRkHBcRRg3q8jDoDKiLMwXiiqEGRiwgSFRUMF8eZQzDhZkJAAgaT\nkEuLENA8KoH3/LFWm01Pdfeq7lTvTvL7PE89e9Xatare7t7db9daVasUEZiZmfW0U7sDMDOz4ckJ\nwszMKjlBmJlZJScIMzOr5ARhZmaVnCDMzKySE4SZmVVygjAzs0pOEGZmVmlkuwMYjP322y8mTpzY\n7jDMzLYpS5Ys+W1EdPS33TadICZOnMjixYvbHYaZ2TZF0kMl27mLyczMKjlBmJlZJScIMzOr5ARh\nZmaVnCDMzKySE4SZmVVygjAzs0pOEGZmVskJwszMKm3Td1K308Q5P6y1/cpz3tBQJGZmzfAZhJmZ\nVXKCMDOzSk4QZmZWyQnCzMwqOUGYmVklJwgzM6vkBGFmZpWcIMzMrFKjCULS+yQtk7RU0mWSdpF0\noKRFklZIukLS6Lztznl9RX5/YpOxmZlZ3xpLEJLGAacDnRHxEmAEcDJwLnB+RBwMPArMyk1mAY/m\n+vPzdmZm1iZNT7UxEthV0pPAbsBa4Ejg7/P784FPAHOBGbkMcBXwfyQpIqLhGM3MBm17nH6nsTOI\niFgDfB74DSkxbASWAI9FxOa82WpgXC6PA1bltpvz9vs2FZ+ZmfWtyS6mvUlnBQcCzwF2B47eCvud\nLWmxpMVdXV2D3Z2ZmfWiyUHq1wK/joiuiHgSuBp4FTBGUnfX1nhgTS6vASYA5Pf3Ah7pudOImBcR\nnRHR2dHR0WD4ZmY7tiYTxG+AaZJ2kyRgOnAPcBPw5rzNTODaXF6Q18nv3+jxBzOz9mlyDGIRabD5\nduCX+VjzgA8BZ0laQRpjuDg3uRjYN9efBcxpKjYzM+tfo1cxRcTHgY/3qH4QOLRi2z8CJzYZj5mZ\nlfOd1GZmVskJwszMKjlBmJlZJScIMzOr5ARhZmaVnCDMzKySE4SZmVVygjAzs0pOEGZmVqnp50GY\nmW0ztsdnOgyGzyDMzKySE4SZmVVygjAzs0pOEGZmVskJwszMKjX5TOpDJN3Zsjwu6UxJ+0i6QdL9\n+XXvvL0kXShphaS7JU1tKjYzM+tfk0+Uuy8ipkTEFODlwCbgGtKT4hZGxCRgIVueHHcMMCkvs4G5\nTcVmZmb9G6oupunAAxHxEDADmJ/r5wPH5/IM4JJIbgXGSNp/iOIzM7MehipBnAxclstjI2JtLq8D\nxubyOGBVS5vVuc7MzNqg8QQhaTRwHPCdnu9FRABRc3+zJS2WtLirq2srRWlmZj0NxRnEMcDtEbE+\nr6/v7jrKrxty/RpgQku78bnuGSJiXkR0RkRnR0dHg2Gbme3YhiJBvJUt3UsAC4CZuTwTuLal/h35\naqZpwMaWrigzMxtijU7WJ2l34Cjgf7VUnwNcKWkW8BBwUq6/DjgWWEG64unUJmMzM7O+NZogIuIP\nwL496h4hXdXUc9sATmsyHjMzK+c7qc3MrFK/CSKPCbxN0sfy+gGSDm0+NDMza6eSM4gvAYeRBpsB\nngAuaiwiMzMbFkrGIF4ZEVMl3QEQEY/mexvMzGw7VnIG8aSkEeQb2iR1AE83GpWZmbVdSYK4kDTJ\n3rMlfQb4OfAvjUZlZmZt128XU0RcKmkJ6dJUAcdHxPLGIzMzs7bqN0FIuhC4PCI8MG1mtgMp6WJa\nAnxU0gOSPi+ps+mgzMys/fpNEBExPyKOBV4B3AecK+n+xiMzM7O2qnMn9cHAC4DnAvc2E46ZmQ0X\nJXdSfzafMfwzsBTojIg3NR6ZmZm1VcmNcg8Ah0XEb5sOxszMho9eE4Skqbn4C+AASQe0vh8RtzcZ\nmJmZtVdfZxDn9fFeAEdu5VjMzGwY6TVBRMRrBrtzSWOArwIvISWVd5GuhLoCmAisBE7K8zsJuID0\n0KBNwDt9lmJm1j4lg9SjJJ0u6aq8vEfSqML9XwD8OCJeAEwGlgNzgIURMQlYmNchPbt6Ul5mA3Nr\nfi1mZrYVlVzmOhd4OWna7y/lcr9/vCXtBRwBXAwQEX+OiMeAGcD8vNl84PhcngFcEsmtwBhJ+9f4\nWszMbCsquYrpFRExuWX9Rkl3FbQ7EOgCvi5pMumO7DOAsRGxNm+zDhiby+OAVS3tV+e6tZiZ2ZAr\nOYN4StLzulckHQQ8VdBuJDAVmBsRLwP+wJbuJOAvz6GO8nBB0mxJiyUt7urqqtPUzMxqKEkQHwBu\nknSzpFuAG4H3F7RbDayOiEV5/SpSwljf3XWUXzfk99cAE1raj891zxAR8yKiMyI6Ozo6CsIwM7OB\nKJnue6GkScAhueq+iPhTQbt1klZJOiQi7iNNF35PXmYC5+TXa3OTBcB7JF0OvBLY2NIVZTu4iXN+\nWGv7lee8oaFIzHYcJdN9n0i6EuluSR8Fpkr6dOElqO8FLs2PKH0QOJV01nKlpFnAQ8BJedvrSJe4\nriBd5npq7a/GzMy2mpJB6v8dEd+RdDjpLODzpKuYXtlfw4i4E6iaHnx6xbYBnFYQj5mZDYGiQer8\n+gbgKxHxQ2B0cyGZmdlwUJIg1kj6v8BbgOsk7VzYzszMtmElf+hPAn4CvD7f6LYP6comMzPbjpVc\nxbQJuLplfS2+ec3MbLvnriIzM6vkBGFmZpVKLnNF0ljgFXn1tojY0Nf2Zma27SuZ7vsk4DbgRNKA\n9SJJb246MDMza6+SM4iPkGZ03QAgqQP4KWluJTMz206VjEHs1KNL6ZHCdmZmtg0rOYP4saSfAJfl\n9bcAP2ouJDMzGw5K7oP4gKQTgMNz1byIuKbZsMzMrN1KZnM9NyI+RMvNci11Zma2nSoZSziqou6Y\nrR2ImZkNL72eQUh6N/CPwEGS7m55a0/gP5oOzMzM2quvLqZvkwaj/5VnPkv6iYj4XaNRmZlZ2/Wa\nICJiI7AReOtAdy5pJfAE6ZkSmyOiU9I+wBXARGAlcFJEPCpJwAWkp8ptAt5Z+NQ6MzNrwFDcz/Ca\niJgSEd1PlpsDLIyIScBCtpydHANMysts0lPrzMysTdpxw9sMYH4uzweOb6m/JJJbgTGS9m9DfGZm\nRtlcTLtL2imXny/pOEmjCvcfwPWSlkianevG5mdKAKwDxubyOGBVS9vVua5nPLMlLZa0uKurqzAM\nMzOrq+QM4mfALpLGAdcDbwe+Ubj/wyNiKqn76DRJR7S+GRFBSiLFImJeRHRGRGdHR0edpmZmVkNJ\nglB+qtwJwJci4kTgxSU7j4g1+XUDcA1wKLC+u+sov3bP87QGmNDSfHyuMzOzNihKEJIOA04Bfpjr\nRhQ02l3Snt1l4HXAUmABMDNvNhO4NpcXAO9QMg3Y2NIVZWZmQ6xksr4zgbOBayJimaSDgJsK2o0F\nrklXrzIS+HZE/FjSL4ArJc0CHiI9YwLgOtIlritIl7meWusrMTOzrapksr5bgFsk7ZbXHwROL2j3\nIDC5ov4RYHpFfQCnFcRsZmZDoOQqpsMk3QPcm9cnS/pS45GZmVlblYxBfBF4PelBQUTEXcARfbYw\nM7NtXtGNchGxqkfVUw3EYmZmw0jJIPUqSX8NRL5B7gxgebNhmZlZu5WcQfwDafB4HOm+hCl4MNnM\nbLtXchXTb0n3QJiZ2Q6krwcG/Rt9TIMREf1e6mpmZtuuvrqYFgNLgF2AqcD9eZkCjG4+NDMza6e+\nHhg0H/7y6NHDI2JzXv8y8O9DE56ZmbVLySD13sCzWtb3yHVmZrYdK7nM9RzgDkk3ASLdJPeJJoMy\nM7P2K7mK6euSfgS8kjRo/aGIWNd4ZGZm1lYlZxCQnuPw6lwO4PvNhGNmZsNFyWR955Dunr4nL6dL\n+pemAzMzs/YqOYM4FpgSEU8DSJoP3AF8uMnAzMysvYom6wPGtJT3qnMASSMk3SHpB3n9QEmLJK2Q\ndIWk0bl+57y+Ir8/sc5xzMxs6ypJEP9KuorpG/nsYQnwmRrH6Dm537nA+RFxMPAoMCvXzwIezfXn\n5+3MzKxN+k0QEXEZMA24GvgucFhEXFGyc0njgTcAX83rAo4ErsqbzAeOz+UZeZ38/vS8vZmZtUHJ\nIPWrgMcjYgHphrkPSnpu4f6/CHwQeDqv7ws81n1XNrCaNEss+XUVQH5/Y97ezMzaoKSLaS6wSdJk\n4CzgAeCS/hpJeiOwISKWDC7E/7Lf2ZIWS1rc1dW1NXdtZmYtShLE5ogIUhfQRRFxEbBnQbtXAcdJ\nWglcTupaugAYI6n76qnxpGdMkF8nAOT39yI/5rRVRMyLiM6I6Ozo6CgIw8zMBqIkQTwh6WzgbcAP\nJe0EjOqvUUScHRHjI2IicDJwY0ScAtwEvDlvNhO4NpcX5HXy+zfmxGRmZm1QkiDeAvwJmJWn2BgP\nfG4Qx/wQcJakFaQxhotz/cXAvrn+LGDOII5hZmaDVDIX0zrgCy3rv6FgDKLHPm4Gbs7lB0lTd/Tc\n5o/AiXX2a2Zmzen1DELSz/PrE5Ie7/k6dCGamVk79PXAoMPza8mAtJmZbWeKZnOVNBU4nDST688j\n4o5GozIzs7YruVHuY6Q7nPcF9gO+IemjTQdmZmbtVXIGcQowOQ8id0//fSfw6SYDMzOz9iq5zPVh\nYJeW9Z3ZcnObmZltp0rOIDYCyyTdQBqDOAq4TdKFABFxeoPxmZlZm5QkiGvy0u3mZkIxM7PhpORG\nufmSdgUOiIj7hiAmMzMbBkquYnoTaVD6x3l9iqQFTQdmZmbtVTJI/QnS1BiPAUTEncBBDcZkZmbD\nQEmCeDIiNvaoe7pySzMz226UDFIvk/T3wAhJk4DTgf9sNiwzM2u3kjOI9wIvJk35/W3SZa9nNhmU\nmZm1X8lVTJuAj+TFzMx2ECVnEAMiaRdJt0m6S9IySZ/M9QdKWiRphaQrJI3O9Tvn9RX5/YlNxWZm\nZv1rLEGQuqSOjIjJwBTgaEnTgHOB8yPiYOBRYFbefhbwaK4/P29nZmZt0liCiOT3eXVUXgI4Ergq\n188Hjs/lGXmd/P50SWoqPjMz61u/YxCSDiQNVE9s3T4ijitoOwJYAhwMXAQ8ADwWEZvzJquBcbk8\nDliV971Z0kbSFOO/LfxazMxsKyq5zPV7wMXA96l5/0NEPAVMkTSGNJ/TC2pH2IOk2cBsgAMOOGCw\nuzMzs16UJIg/RsSFgzlIRDwm6SbgMGCMpJH5LGI8W6YOXwNMAFZLGgnsBTxSsa95wDyAzs7OGExc\nZmbWu5IxiAskfVzSYZKmdi/9NZLUkc8cyJP9HQUsB24C3pw3mwlcm8sL8jr5/RsjwgnAzKxNSs4g\nXgq8nTS43N3F1D3Y3Jf9gfl5HGIn4MqI+IGke4DLJX0auIPUfUV+/aakFcDvgJNrfSVmZrZVlSSI\nE4GDIuLPdXYcEXcDL6uof5A0+V/P+j/mY5mZ2TBQ0sW0FBjTdCBmZja8lJxBjAHulfQL0s1vQNll\nrmZmtu0qSRAfbzwKMzMbdkom67tlKAIxM7PhpeRO6idIVy0BjCZNmfGHiHhWk4GZmVl7lZxB7Nld\nznMjzQCmNRmUmZm1X63J+vIEfN8DXt9QPGZmNkyUdDGd0LK6E9AJ/LGxiMzMbFgouYrpTS3lzcBK\nUjeTmZltx0rGIE4dikDMzGx46TVBSPpYH+0iIj7VQDxmZjZM9HUG8YeKut1JjwbdF3CCMDPbjvWa\nICLivO6ypD2BM4BTgcuB83prZ2Zm24c+xyAk7QOcBZxCel701Ih4dCgCMzOz9uprDOJzwAmkp7e9\nNCJ+P2RRmZlZ2/V1o9z7gecAHwUelvR4Xp6Q9PjQhGdmZu3Sa4KIiJ0iYteI2DMintWy7FkyD5Ok\nCZJuknSPpGWSzsj1+0i6QdL9+XXvXC9JF0paIenukseamplZc2pNtVHTZuD9EfEi0txNp0l6ETAH\nWBgRk4CFeR3gGGBSXmYDcxuMzczM+tFYgoiItRFxey4/ASwHxpHuwp6fN5sPHJ/LM4BL8nxPtwJj\nJO3fVHxmZta3Js8g/kLSRNLzqRcBYyNibX5rHTA2l8cBq1qarc51Pfc1W9JiSYu7uroai9nMbEfX\neIKQtAfwXeDMiHjG4HZEBFueNVEkIuZFRGdEdHZ0dGzFSM3MrFWjCULSKFJyuDQirs7V67u7jvLr\nhly/BpjQ0nx8rjMzszZoLEHkhwtdDCyPiC+0vLUAmJnLM4FrW+rfka9mmgZsbOmKMjOzIVYy3fdA\nvQp4O/BLSXfmug8D5wBXSpoFPASclN+7DjgWWAFsIk3rYWZmbdJYgoiInwPq5e3pFdsHcFpT8ZiZ\nWT1DchWTmZlte5wgzMyskhOEmZlVcoIwM7NKThBmZlbJCcLMzCo5QZiZWSUnCDMzq+QEYWZmlZwg\nzMyskhOEmZlVcoIwM7NKThBmZlbJCcLMzCo5QZiZWaUmnyj3NUkbJC1tqdtH0g2S7s+ve+d6SbpQ\n0gpJd0ua2lRcZmZWpskziG8AR/eomwMsjIhJwMK8DnAMMCkvs4G5DcZlZmYFGksQEfEz4Hc9qmcA\n83N5PnB8S/0lkdwKjJG0f1OxmZlZ/4Z6DGJsRKzN5XXA2FweB6xq2W51rjMzszZp2yB1fgZ11G0n\nabakxZIWd3V1NRCZmZnB0CeI9d1dR/l1Q65fA0xo2W58rvsvImJeRHRGRGdHR0ejwZqZ7ciGOkEs\nAGbm8kzg2pb6d+SrmaYBG1u6oszMrA1GNrVjSZcBfwvsJ2k18HHgHOBKSbOAh4CT8ubXAccCK4BN\nwKlNxWVmZmUaSxAR8dZe3ppesW0ApzUVi5mZ1ec7qc3MrJIThJmZVXKCMDOzSk4QZmZWyQnCzMwq\nOUGYmVklJwgzM6vkBGFmZpWcIMzMrJIThJmZVXKCMDOzSk4QZmZWyQnCzMwqOUGYmVklJwgzM6s0\nrBKEpKMl3SdphaQ57Y7HzGxHNmwShKQRwEXAMcCLgLdKelF7ozIz23ENmwQBHAqsiIgHI+LPwOXA\njDbHZGa2wxpOCWIcsKplfXWuMzOzNmjsmdRNkTQbmJ1Xfy/pvq18iP2A327t9jq3fcce5m0bOfZ2\n/v1u57Edd4vBfM4K2/bafpCeW7RVRAyLBTgM+EnL+tnA2W2IY3G72m+rx3bcO86xHfe2dezBLsOp\ni+kXwCRJB0oaDZwMLGhzTGZmO6xh08UUEZslvQf4CTAC+FpELGtzWGZmO6xhkyAAIuI64Lo2hzGv\nje231WM77h3n2I572zr2oCj3cZmZmT3DcBqDMDOzYcQJosVgpvqQ9DVJGyQtHcBxJ0i6SdI9kpZJ\nOqNG210k3Sbprtz2kwM4/ghJd0j6wQDarpT0S0l3Slpcs+0YSVdJulfSckmH1Wh7SD5m9/K4pDNr\ntH9f/n4tlXSZpF1qtD0jt1tWcsyqz4akfSTdIOn+/Lp3jbYn5mM/LalzAMf+XP6e3y3pGkljarT9\nVG53p6TrJT2ntG3Le++XFJL2qxn3JyStafmZH1vn2JLem7/uZZI+W/PYV7Qcd6WkO2u0nSLp1u7f\nEUmH1mg7WdL/y79j35f0rN7ibkS7Lp8abgtpYPwB4CBgNHAX8KIa7Y8ApgJLB3Ds/YGpubwn8KvS\nYwMC9sjlUcAiYFrN458FfBv4wQBiXwnsN8Dv+Xzgf+TyaGDMIH5264DnFm4/Dvg1sGtevxJ4Z2Hb\nlwBLgd1IY3g/BQ6u+9kAPgvMyeU5wLk12r4QOAS4GegcwLFfB4zM5XNrHvtZLeXTgS+Xts31E0gX\nojzU1+eml2N/Avingp9RVdvX5J/Vznn92XXa93j/POBjNY59PXBMLh8L3Fyj7S+Av8nldwGfGsjv\nyEAXn0FsMaipPiLiZ8DvBnLgiFgbEbfn8hPAcgrvIo/k93l1VF6KB5YkjQfeAHy1VtCDJGkv0i/E\nxQAR8eeIeGyAu5sOPBARD9VoMxLYVdJI0h/7hwvbvRBYFBGbImIzcAtwQl8NevlszCAlSPLr8aVt\nI2J5RBTdINpL++tz7AC3AuNrtH28ZXV3evms9fH7cD7wwd7aFbTvVy9t3w2cExF/yttsGMixJQk4\nCbisRtsAuv/z34tePmu9tH0+8LNcvgH4773F3QQniC2GxVQfkiYCLyOdCZS2GZFPeTcAN0REcVvg\ni6Rf2KdrtGkVwPWSlijd5V7qQKAL+Hru3vqqpN0HGMPJ9PILWyUi1gCfB34DrAU2RsT1hc2XAq+W\ntK+k3Uj/EU6oGS/A2IhYm8vrgLED2MfW8C7gR3UaSPqMpFXAKcDHarSbAayJiLvqhfgM78ldXF/r\nrVuuF88n/dwWSbpF0isGePxXA+sj4v4abc4EPpe/Z58n3QRcahlb/lE9kYF91gbMCWIYkbQH8F3g\nzB7/qfUpIp6KiCmk/wQPlfSSwuO9EdgQEUsGFHByeERMJc3Ce5qkIwrbjSSdTs+NiJcBfyB1tdSi\ndFPlccB3arTZm/RLdyDwHGB3SW8raRsRy0ndMtcDPwbuBJ6qGXbPfQY1zvq2FkkfATYDl9ZpFxEf\niYgJud17Co+1G/BhaiSUCnOB5wFTSIn9vBptRwL7ANOADwBX5rOBut5KjX9GsncD78vfs/eRz5oL\nvQv4R0lLSN3Pf6557EFxgthiDc/MzuNz3ZCQNIqUHC6NiKsHso/cRXMTcHRhk1cBx0laSepSO1LS\nt2oec01+3QBcQ+qqK7EaWN1ytnMVKWHUdQxwe0Ssr9HmtcCvI6IrIp4Ergb+urRxRFwcES+PiCOA\nR0ljRnWtl7Q/QH7ttcujCZLeCbwROCUnqIG4lPIuj+eREvJd+fM2Hrhd0l+VHiwi1ud/hp4GvkL5\nZw3S5+3q3CV7G+mMuddB8iq5O/IE4Io67YCZpM8YpH9kiuOOiHsj4nUR8XJSYnqg5rEHxQlii7ZN\n9ZH/k7kYWB4RX6jZtqP7KhRJuwJHAfeWtI2IsyNifERMJH29N0ZE0X/S+Xi7S9qzu0wa/Cy6iisi\n1gGrJB2Sq6YD95Qeu8VA/qP7DTBN0m75ez+dNO5TRNKz8+sBpD8Y3655fEifrZm5PBO4dgD7GBBJ\nR5O6FY+LiE01205qWZ1B+WftlxHx7IiYmD9vq0kXZqyrcez9W1b/jsLPWvY90kA1kp5Puiii7gR4\nrwXujYjVNds9DPxNLh8JFHdPtXzWdgI+Cny55rEHZyhHxIf7QupP/hUpS3+kZtvLSKe9T5I+/LNq\ntD2c1MVwN6nL4k7g2MK2/w24I7ddSi9XVxTs52+peRUT6Yqvu/KybADfsynA4hz794C9a7bfHXgE\n2GsAX+8nSX/clgLfJF/dUtj230nJ7C5g+kA+G8C+wELSH4ufAvvUaPt3ufwnYD0tk1wWtl9BGm/r\n/qz1diVSVdvv5u/Z3cD3gXED+X2gn6vfejn2N4Ff5mMvAPav0XY08K0c++3AkXWOneu/AfzDAH7W\nhwNL8udlEfDyGm3PIP1N+hVwDvnm5qFafCe1mZlVcheTmZlVcoIwM7NKThBmZlbJCcLMzCo5QZiZ\nWSUnCLM2kvT7/rcyaw8nCDMzqzSsHjlqNpzlu8WvJE0TMQL4FOmmsy8Ae5DuzH1nRKyV9DzgIqAD\n2AT8z4i4V9KBpDuv92AI7542GwifQZiVOxp4OCImR8RLSJP1/Rvw5khz5XwN+Ezedh7w3lz/T8CX\ncv0FpAkKX0q6a9Zs2PKd1GaF8hw+15Mma/sBaaK+/wQezJuMIP3RP4E0lXnrMxt2jogXSnoE+KuI\neDI/HezhiNhjqL4GszrcxWRWKCJ+JWkqac6uTwM3Assi4hmPSs1/+B+LNAV75a6ajdRs63AXk1mh\n/PzlTRHxLeBzwCuBDuVnaUsaJenFkZ7l8WtJJ+Z6SZqcd/MfpJlzIT1wx2zYcheTWSFJryclhqdJ\nM26+m/TAnQtJj5IcCXwxIr6SB6Pnkp43Pgq4PCL+uWKQ+kx3Mdlw5QRhZmaV3MVkZmaVnCDMzKyS\nE4SZmVVygjAzs0pOEGZmVskJwszMKjlBmJlZJScIMzOr9P8Bx5GeVQ/7bcQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF8XgW-bzVsq",
        "colab_type": "text"
      },
      "source": [
        "As u can see, with intial seeds as 1,7,17,18 are the only ones, in which the policy could successfully solve the environment within 1000 episodes. Hence the speed at which our policies learn varies greatly with the initial random parameters."
      ]
    }
  ]
}